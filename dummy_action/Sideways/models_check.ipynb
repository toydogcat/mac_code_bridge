{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Place 位置\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name : .DS_Store,\t size : 10244, len : 9\n",
      "name : BlazePose,\t size : 160, len : 9\n",
      "name : movenet,\t size : 192, len : 7\n",
      "name : movenet.tflite,\t size : 4758512, len : 14\n",
      "name : movenet_multipose_lightning,\t size : 192, len : 27\n",
      "name : movenet_multipose_lightning_float16.tflite,\t size : 9585276, len : 42\n",
      "name : movenet_t,\t size : 192, len : 9\n",
      "name : movenet_t.tflite,\t size : 12584128, len : 16\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "path = Path(\"../utils/models/\")\n",
    "\n",
    "# .stat().st_size\n",
    "for item in sorted(path.glob(\"*\")):\n",
    "    print(f\"name : {item.name},\\t size : {item.stat().st_size}, len : {len(item.name)}\")\n",
    "\n",
    "# len(item.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF Lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-22 11:03:54.468185: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-22 11:04:37.879012: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't receive frame (stream end?). Exiting ...\n",
      "count : 54, avg time : 31, fps : 31\n"
     ]
    }
   ],
   "source": [
    "#################################################\n",
    "##  Records \n",
    "##  macbook pro intel cpu\n",
    "##    single_thunder : avg time : 87, fps : 11\n",
    "##    single_light   : avg time : 29, fps : 33\n",
    "##    multi_light    : avg time : 81, fps : 12\n",
    "##  macmini m2 pro cpu\n",
    "##    single_thunder : avg time : 56, fps : 17\n",
    "##    single_light   : avg time : 13, fps : 75\n",
    "##    multi_light    : avg time : 36, fps : 27\n",
    "## \n",
    "\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "video_path  = \"Upright row bilateral dumbbells.mp4\"\n",
    "model_index = 1\n",
    "show_index  = 10\n",
    "show_flag   = False\n",
    "\n",
    "model_type_list = [\n",
    "    \"single_thunder\",\n",
    "    \"single_light\",\n",
    "    \"multi_light\"\n",
    "]\n",
    "model_type = model_type_list[model_index]\n",
    "\n",
    "if model_type == \"single_thunder\":\n",
    "    model_place = \"../utils/models/movenet_t.tflite\"\n",
    "elif model_type == \"single_light\":\n",
    "    model_place = \"../utils/models/movenet.tflite\"\n",
    "elif model_type == \"multi_light\":\n",
    "    model_place = \"../utils/models/movenet_multipose_lightning_float16.tflite\"\n",
    "\n",
    "\n",
    "if model_type == \"single_thunder\" or model_type == \"single_light\":\n",
    "    interpreter = tf.lite.Interpreter(model_path=model_place)\n",
    "    interpreter.allocate_tensors()\n",
    "    input_details  = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "elif model_type == \"multi_light\":\n",
    "    target_size = 256     # 32 倍數\n",
    "\n",
    "    interpreter = tf.lite.Interpreter(model_path=model_place)\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    is_dynamic_shape_model = input_details[0]['shape_signature'][2] == -1\n",
    "    if is_dynamic_shape_model:\n",
    "        input_tensor_index = input_details[0]['index']\n",
    "        input_shape = (1, target_size, target_size, 3)\n",
    "        interpreter.resize_tensor_input(input_tensor_index, input_shape, strict=True)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "count      = 0\n",
    "count_time = 0\n",
    "while(True):\n",
    "    # 擷取影像\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "        break\n",
    "\n",
    "    # frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    heigh, width, _ = frame.shape\n",
    "    # 裁切為正方形\n",
    "    if heigh < width:\n",
    "        x = int((width-heigh)/2); y = 0\n",
    "        w = heigh; h = heigh; width = heigh\n",
    "        frame = frame[y:y+h, x:x+w]\n",
    "    elif heigh > width:\n",
    "        x = 0; y = int((heigh-width)/2)\n",
    "        w = width; h = width; heigh = width\n",
    "        frame = frame[y:y+h, x:x+w]\n",
    "\n",
    "    start_time = time.time()\n",
    "    if model_type == \"single_thunder\" or model_type == \"single_light\":\n",
    "        # thunder 256, light 192\n",
    "        image = cv2.resize(frame, (input_details[0]['shape'][1], input_details[0]['shape'][1]), interpolation=cv2.INTER_AREA)[:,:,::-1]\n",
    "        image = tf.expand_dims(image, axis=0)\n",
    "        interpreter.set_tensor(input_details[0]['index'], tf.cast(image, dtype=tf.uint8).numpy())\n",
    "        interpreter.invoke()\n",
    "        target_keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
    "        keypoints_with_scores = target_keypoints_with_scores[0][0]\n",
    "    elif model_type == \"multi_light\":\n",
    "        image_target = cv2.resize(frame, (target_size, target_size), interpolation=cv2.INTER_AREA)[:,:,::-1]\n",
    "        input_tensor = tf.expand_dims(image_target, axis=0)\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_tensor.numpy())\n",
    "        interpreter.invoke()\n",
    "        multi_keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
    "        target_keypoints_with_scores = multi_keypoints_with_scores[0][0]\n",
    "        keypoints_with_scores = target_keypoints_with_scores[:51].reshape((17,3))\n",
    "        \n",
    "    \n",
    "    # Draw\n",
    "    if count == show_index and show_flag:\n",
    "        for keypoint in keypoints_with_scores:\n",
    "            y_coordinate = int( keypoint[0] * heigh )\n",
    "            x_coordinate = int( keypoint[1] * width )\n",
    "            score = keypoint[2]\n",
    "\n",
    "            if score > 0.8:\n",
    "                cv2.circle(frame, (x_coordinate, y_coordinate), 2, (255,0,0), 2)\n",
    "            elif score > 0.4:\n",
    "                cv2.circle(frame, (x_coordinate, y_coordinate), 2, (255,255,0), 2)\n",
    "            else:\n",
    "                cv2.circle(frame, (x_coordinate, y_coordinate), 2, (0,0,255), 2)\n",
    "        \n",
    "                \n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(frame[:,:,::-1])\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    end_time = time.time()\n",
    "    count      += 1\n",
    "    count_time += (end_time - start_time)\n",
    "    \n",
    "    # print(f\"time : {int((end_time - start_time)*1000)}\")\n",
    "\n",
    "cap.release()\n",
    "avg_time = count_time / count\n",
    "\n",
    "print(f\"count : {count}, avg time : {int(avg_time * 1000)}, fps : {int(1/avg_time)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't receive frame (stream end?). Exiting ...\n",
      "count : 54, avg time : 53, fps : 18\n"
     ]
    }
   ],
   "source": [
    "#################################################\n",
    "##  Records \n",
    "##  macbook pro intel cpu\n",
    "##    single_thunder : avg time : 81, fps : 12\n",
    "##    single_light   : avg time : 52, fps : 18\n",
    "##    multi_light    : avg time : 97, fps : 10\n",
    "##  macmini m2 pro cpu\n",
    "##    single_thunder : avg time : 49, fps : 20\n",
    "##    single_light   : avg time : 45, fps : 21\n",
    "##    multi_light    : avg time : 60, fps : 16\n",
    "## \n",
    "\n",
    "\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "video_path  = \"Upright row bilateral dumbbells.mp4\"\n",
    "model_index = 1\n",
    "show_index  = 10\n",
    "show_flag   = False\n",
    "\n",
    "model_type_list = [\n",
    "    \"single_thunder\",\n",
    "    \"single_light\",\n",
    "    \"multi_light\"\n",
    "]\n",
    "model_type = model_type_list[model_index]\n",
    "\n",
    "if model_type == \"single_thunder\":\n",
    "    model_place = \"../utils/models/movenet_t\"\n",
    "elif model_type == \"single_light\":\n",
    "    model_place = \"../utils/models/movenet\"\n",
    "elif model_type == \"multi_light\":\n",
    "    model_place = \"../utils/models/movenet_multipose_lightning\"\n",
    "\n",
    "# print(f\"model path : {model_place}\")\n",
    "\n",
    "\n",
    "model_load = tf.saved_model.load(model_place)\n",
    "model = model_load.signatures['serving_default']\n",
    "if model_type == \"single_thunder\" or model_type == \"single_light\":\n",
    "    _, target_height, target_width, _ = model.inputs[0].shape\n",
    "elif model_type == \"multi_light\":\n",
    "    target_height = 256; target_width = 256\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "count      = 0\n",
    "count_time = 0\n",
    "while(True):\n",
    "    # 擷取影像\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "        break\n",
    "\n",
    "    # frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    heigh, width, _ = frame.shape\n",
    "    # 裁切為正方形\n",
    "    if heigh < width:\n",
    "        x = int((width-heigh)/2); y = 0\n",
    "        w = heigh; h = heigh; width = heigh\n",
    "        frame = frame[y:y+h, x:x+w]\n",
    "    elif heigh > width:\n",
    "        x = 0; y = int((heigh-width)/2)\n",
    "        w = width; h = width; heigh = width\n",
    "        frame = frame[y:y+h, x:x+w]\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    image_resize = cv2.resize(frame, (target_width, target_height), interpolation=cv2.INTER_AREA)[:,:,::-1]\n",
    "    image_r0 = tf.expand_dims(image_resize, axis=0)\n",
    "    input_image = tf.cast(image_r0, dtype=tf.int32)\n",
    "    \n",
    "    if model_type == \"single_thunder\" or model_type == \"single_light\":\n",
    "        target_keypoints_with_scores = model(input_image)['output_0']\n",
    "        keypoints_with_scores = target_keypoints_with_scores[0][0].numpy()\n",
    "    elif model_type == \"multi_light\":\n",
    "        # input_image /= 255\n",
    "        multi_keypoints_with_scores = model(input_image)['output_0']\n",
    "        target_keypoints_with_scores = multi_keypoints_with_scores[0][0].numpy()\n",
    "        keypoints_with_scores = target_keypoints_with_scores[:51].reshape((17,3))\n",
    "        \n",
    "    # Draw\n",
    "    if count == show_index and show_flag:\n",
    "        for keypoint in keypoints_with_scores:\n",
    "            y_coordinate = int( keypoint[0] * heigh )\n",
    "            x_coordinate = int( keypoint[1] * width )\n",
    "            score = keypoint[2]\n",
    "\n",
    "            if score > 0.8:\n",
    "                cv2.circle(frame, (x_coordinate, y_coordinate), 2, (255,0,0), 2)\n",
    "            elif score > 0.4:\n",
    "                cv2.circle(frame, (x_coordinate, y_coordinate), 2, (255,255,0), 2)\n",
    "            else:\n",
    "                cv2.circle(frame, (x_coordinate, y_coordinate), 2, (0,0,255), 2)\n",
    "        \n",
    "                \n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(frame[:,:,::-1])\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    end_time = time.time()\n",
    "    count      += 1\n",
    "    count_time += (end_time - start_time)\n",
    "    \n",
    "    # print(f\"time : {int((end_time - start_time)*1000)}\")\n",
    "\n",
    "cap.release()\n",
    "avg_time = count_time / count\n",
    "\n",
    "print(f\"count : {count}, avg time : {int(avg_time * 1000)}, fps : {int(1/avg_time)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??tf.lite.Interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4b68bd7c877d9e56a020031c6d7861900a455e083e943b385babf9aba0749e24"
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 ('yolo-tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
